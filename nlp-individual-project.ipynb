{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13929563,"sourceType":"datasetVersion","datasetId":8876624}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from transformers import T5ForConditionalGeneration, T5Tokenizer\nimport torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T09:48:56.798942Z","iopub.execute_input":"2025-11-30T09:48:56.799351Z","iopub.status.idle":"2025-11-30T09:48:56.803760Z","shell.execute_reply.started":"2025-11-30T09:48:56.799300Z","shell.execute_reply":"2025-11-30T09:48:56.802893Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Current device used: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T09:48:56.804920Z","iopub.execute_input":"2025-11-30T09:48:56.805171Z","iopub.status.idle":"2025-11-30T09:48:56.822382Z","shell.execute_reply.started":"2025-11-30T09:48:56.805150Z","shell.execute_reply":"2025-11-30T09:48:56.821603Z"}},"outputs":[{"name":"stdout","text":"Current device used: cuda\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"MODEL_NAME = \"google/flan-t5-base\"\ntokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)\nmodel = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)\nmodel.eval()\nmodel.to(device)\n\n# check if model is successfully loaded\nprint(f\"Model {MODEL_NAME} has been loaded successfully\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T09:48:56.823569Z","iopub.execute_input":"2025-11-30T09:48:56.823837Z","iopub.status.idle":"2025-11-30T09:48:58.185848Z","shell.execute_reply.started":"2025-11-30T09:48:56.823813Z","shell.execute_reply":"2025-11-30T09:48:58.185151Z"}},"outputs":[{"name":"stdout","text":"Model google/flan-t5-base has been loaded successfully\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"VERBALIZER = {\n    \"entailment\": \"true\",\n    \"contradiction\": \"false\",\n    \"neutral\": \"neither\",\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T09:48:58.187845Z","iopub.execute_input":"2025-11-30T09:48:58.188147Z","iopub.status.idle":"2025-11-30T09:48:58.192371Z","shell.execute_reply.started":"2025-11-30T09:48:58.188119Z","shell.execute_reply":"2025-11-30T09:48:58.191406Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"verbalizer_words = list(VERBALIZER.values())\nverbalizer_token_ids = {}\n\nfor label, word in VERBALIZER.items():\n    token_id = tokenizer.encode(word, add_special_tokens=False)[0]\n    verbalizer_token_ids[label] = token_id\n    \nprint(\"\\nVerbalizer Token ID projection has created:\")\nprint(verbalizer_token_ids)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T09:48:58.193202Z","iopub.execute_input":"2025-11-30T09:48:58.193472Z","iopub.status.idle":"2025-11-30T09:48:58.212204Z","shell.execute_reply.started":"2025-11-30T09:48:58.193450Z","shell.execute_reply":"2025-11-30T09:48:58.211364Z"}},"outputs":[{"name":"stdout","text":"\nVerbalizer Token ID projection has created:\n{'entailment': 1176, 'contradiction': 6136, 'neutral': 7598}\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"target_ids = list(verbalizer_token_ids.values())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T09:48:58.213047Z","iopub.execute_input":"2025-11-30T09:48:58.213291Z","iopub.status.idle":"2025-11-30T09:48:58.227349Z","shell.execute_reply.started":"2025-11-30T09:48:58.213271Z","shell.execute_reply":"2025-11-30T09:48:58.226567Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"def predict_nli_prompting(premise: str, hypothesis: str, model, tokenizer, verbalizer_token_ids, device) -> str:\n    query = f'Premise：“{premise}”. Hypothesis：“{hypothesis}”. The relationship between them is：'\n    \n    input_ids = tokenizer(query, return_tensors=\"pt\", truncation=True, max_length=512).input_ids\n    input_ids = input_ids.to(device)\n    \n    with torch.no_grad():\n        output = model.generate(\n            input_ids,\n            max_new_tokens=1,\n            output_scores=True,\n            return_dict_in_generate=True\n        )\n    logits = output.scores[0].squeeze() \n    \n    probabilities = torch.softmax(logits, dim=-1)\n    \n    max_prob = -1.0\n    predicted_label = \"neutral\" \n    \n    for label, token_id in verbalizer_token_ids.items():\n        prob = probabilities[token_id].item() \n        \n        if prob > max_prob:\n            max_prob = prob\n            predicted_label = label\n            \n    return predicted_label","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T03:04:47.480304Z","iopub.execute_input":"2025-12-01T03:04:47.481038Z","iopub.status.idle":"2025-12-01T03:04:47.487491Z","shell.execute_reply.started":"2025-12-01T03:04:47.481010Z","shell.execute_reply":"2025-12-01T03:04:47.486622Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import json\nimport os\nfrom typing import List, Dict","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T03:07:36.284919Z","iopub.execute_input":"2025-12-01T03:07:36.285675Z","iopub.status.idle":"2025-12-01T03:07:36.289568Z","shell.execute_reply.started":"2025-12-01T03:07:36.285639Z","shell.execute_reply":"2025-12-01T03:07:36.288755Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"LABEL_MAPPING = {\n    \"entailment\": \"entailment\",\n    \"contradiction\": \"contradiction\",\n    \"neutral\": \"neutral\"\n}\n\ndef load_jsonl_data(file_path: str) -> List[Dict]:\n    data = []\n    print(f\"Data Loading: {file_path}\")\n    with open(file_path, 'r', encoding='utf-8') as f:\n        for line in f:\n            try:\n                item = json.loads(line.strip())\n                if item.get(\"gold_label\") in LABEL_MAPPING:\n                    data.append({\n                        'premise': item['sentence1'],\n                        'hypothesis': item['sentence2'],\n                        'gold_label': item['gold_label']\n                    })\n            except json.JSONDecodeError as e:\n                print(f\"Skip invalid JSON row: {e}\")\n    return data\n\ndef run_evaluation(data: List[Dict], set_name: str, model, tokenizer, verbalizer_token_ids):\n    correct_predictions = 0\n    total_samples = len(data)\n\n    print(f\"\\n--- Evaluation Starting {set_name} (Total samples: {total_samples}) ---\")\n\n    for i, sample in enumerate(data):\n        predicted_label = predict_nli_prompting(\n            sample['premise'], \n            sample['hypothesis'], \n            model, \n            tokenizer, \n            verbalizer_token_ids,\n            \n        )\n\n        if predicted_label == sample['gold_label']:\n            correct_predictions += 1\n        \n        if (i + 1) % 500 == 0:\n            print(f\"Have processed {i + 1}/{total_samples} samples...\")\n\n    accuracy = correct_predictions / total_samples\n    print(f\"--- Evaluation Finished {set_name} ---\")\n    print(f\"Accuracy: {accuracy:.4f} ({correct_predictions} / {total_samples})\")\n    \n    return accuracy\n\n\nMATCHED_FILE = \"/kaggle/input/nlpindividualproject/dev_matched_sampled-1.jsonl\"\nMISMATCHED_FILE = \"/kaggle/input/nlpindividualproject/dev_mismatched_sampled-1.jsonl\"\n\nmatched_data = load_jsonl_data(MATCHED_FILE)\nmismatched_data = load_jsonl_data(MISMATCHED_FILE)\n\nmatched_acc = run_evaluation(matched_data, \"Matched Set\", model, tokenizer, verbalizer_token_ids)\nmismatched_acc = run_evaluation(mismatched_data, \"Mismatched Set\", model, tokenizer, verbalizer_token_ids)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T03:16:42.223124Z","iopub.execute_input":"2025-12-01T03:16:42.223433Z","iopub.status.idle":"2025-12-01T03:16:42.301684Z","shell.execute_reply.started":"2025-12-01T03:16:42.223409Z","shell.execute_reply":"2025-12-01T03:16:42.300677Z"}},"outputs":[{"name":"stdout","text":"Data Loading: /kaggle/input/nlpindividualproject/dev_matched_sampled-1.jsonl\nData Loading: /kaggle/input/nlpindividualproject/dev_mismatched_sampled-1.jsonl\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_47/1316903858.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0mmismatched_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_jsonl_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMISMATCHED_FILE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m \u001b[0mmatched_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_evaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatched_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Matched Set\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbalizer_token_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0mmismatched_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_evaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmismatched_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Mismatched Set\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbalizer_token_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"],"ename":"NameError","evalue":"name 'model' is not defined","output_type":"error"}],"execution_count":19},{"cell_type":"code","source":"from datasets import load_dataset\n\nhallucination_dataset = load_dataset(\"potsawee/wiki_bio_gpt3_hallucination\", split=\"evaluation\")\n\n\ndef preprocess_hallucination_data(dataset) -> List[Dict]:\n    processed_samples = []\n    \n    for entry in dataset:\n        premise = entry['wiki_bio_text']\n        gpt3_sentences = entry['gpt3_sentences']\n        annotations = entry['annotation']\n        \n        for sentence, annotation in zip(gpt3_sentences, annotations):\n            binary_label = 'Factual' if annotation == 0.0 else 'Non-Factual'    \n            processed_samples.append({\n                'premise': premise,\n                'hypothesis': sentence,\n                'gold_label': binary_label \n            })\n            \n    return processed_samples\n\nif hallucination_dataset:\n    evaluation_data = preprocess_hallucination_data(hallucination_dataset)\n    print(f\"Dataset preprocessing finished，total {len(evaluation_data)} sentence evaluation samples.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T16:33:01.857718Z","iopub.execute_input":"2025-11-30T16:33:01.858004Z","iopub.status.idle":"2025-11-30T16:33:07.398924Z","shell.execute_reply.started":"2025-11-30T16:33:01.857983Z","shell.execute_reply":"2025-11-30T16:33:07.398375Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fdd90d926b7f4fde9a5746a50ee2b071"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/evaluation-00000-of-00001-e91191b8f(…):   0%|          | 0.00/2.56M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8106897f43114158b8feb8334f5ff108"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating evaluation split:   0%|          | 0/238 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"01ab99ba81624cfabd3394edaedf3edf"}},"metadata":{}},{"name":"stdout","text":"Dataset preprocessing finished，total 1908 sentence evaluation samples.\n","output_type":"stream"}],"execution_count":110},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, precision_recall_fscore_support","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T09:51:32.131264Z","iopub.execute_input":"2025-11-30T09:51:32.131545Z","iopub.status.idle":"2025-11-30T09:51:32.135546Z","shell.execute_reply.started":"2025-11-30T09:51:32.131526Z","shell.execute_reply":"2025-11-30T09:51:32.134764Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"def calculate_metrics(y_true: list, y_pred: list):\n    accuracy = accuracy_score(y_true, y_pred)\n    precision, recall, f1, _ = precision_recall_fscore_support(\n        y_true, \n        y_pred, \n        average='binary', \n        pos_label='Non-Factual' \n    )\n\n    return {\n        'accuracy': accuracy,\n        'precision': precision,\n        'recall': recall,\n        'f1': f1\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T16:39:22.698833Z","iopub.execute_input":"2025-11-30T16:39:22.699522Z","iopub.status.idle":"2025-11-30T16:39:22.703500Z","shell.execute_reply.started":"2025-11-30T16:39:22.699498Z","shell.execute_reply":"2025-11-30T16:39:22.702794Z"}},"outputs":[],"execution_count":116},{"cell_type":"code","source":"import time","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T09:51:32.152176Z","iopub.execute_input":"2025-11-30T09:51:32.152453Z","iopub.status.idle":"2025-11-30T09:51:32.164741Z","shell.execute_reply.started":"2025-11-30T09:51:32.152426Z","shell.execute_reply":"2025-11-30T09:51:32.163942Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"def nli_to_hallucination_label(nli_label: str) -> str:\n    if nli_label == 'entailment':\n        return 'Factual'\n    else:\n        return 'Non-Factual'\n\ndef run_hallucination_evaluation(data: List[Dict], model, tokenizer, verbalizer_token_ids, device):\n    y_true = []\n    y_pred = []\n    total_samples = len(data)\n    \n    print(f\"\\n--- Starting Hallucination Detection (Total Samples: {total_samples}) ---\")\n\n    start_time = time.time()\n    detailed_results = []\n    \n    for i, sample in enumerate(data):\n        nli_predicted_label = predict_nli_prompting(\n            sample['premise'], \n            sample['hypothesis'], \n            model, \n            tokenizer, \n            verbalizer_token_ids,\n            device \n        )\n        \n        binary_predicted_label = nli_to_hallucination_label(nli_predicted_label)\n\n        # --------------------------case sample get (test)-----------------------\n        detailed_results.append({\n            'premise': sample['premise'],\n            'hypothesis': sample['hypothesis'],\n            'gold_label': sample['gold_label'],\n            'nli_prediction': nli_predicted_label, \n            'binary_prediction': binary_predicted_label,\n            'is_correct': (binary_predicted_label == sample['gold_label'])\n        })\n        # --------------------------test end----------------------------------\n        \n        y_pred.append(binary_predicted_label)\n        y_true.append(sample['gold_label'])\n        \n        if (i + 1) % 500 == 0:\n            elapsed = time.time() - start_time\n            print(f\"Have processed {i + 1}/{total_samples} samples... Time cost: {elapsed:.2f}s\")\n            \n    metrics = calculate_metrics(y_true, y_pred)\n    \n    print(\"--- Hallucination Detection Finished ---\")\n    \n    return detailed_results, metrics\n\n\nhallucination_results, hallucination_metrics = run_hallucination_evaluation(evaluation_data, model, tokenizer, verbalizer_token_ids, device)\n\nprint(\"\\nFlan-T5 Hallucination Detection Results:\")\nfor metric, value in hallucination_metrics.items():\n    print(f\"{metric.capitalize()}: {value:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T10:01:43.759356Z","iopub.execute_input":"2025-11-30T10:01:43.759955Z","iopub.status.idle":"2025-11-30T10:02:56.209508Z","shell.execute_reply.started":"2025-11-30T10:01:43.759931Z","shell.execute_reply":"2025-11-30T10:02:56.208846Z"}},"outputs":[{"name":"stdout","text":"\n--- Starting Hallucination Detection (Total Samples: 1908) ---\nHave processed 500/1908 samples... Time cost: 19.12s\nHave processed 1000/1908 samples... Time cost: 37.87s\nHave processed 1500/1908 samples... Time cost: 56.88s\n--- Hallucination Detection Finished ---\n\nFlan-T5 Hallucination Detection Results:\nAccuracy: 0.3029\nPrecision: 1.0000\nRecall: 0.3029\nF1: 0.4650\n","output_type":"stream"}],"execution_count":41},{"cell_type":"code","source":"# case： label is Non-Factual， but model predicted Factual\ncase_study_samples = []\n\nfor result in hallucination_results:\n    if result['gold_label'] == 'Non-Factual' and result['binary_prediction'] == 'Factual':\n        case_study_samples.append(result)\n        break \n\nif case_study_samples:\n    failed_case = case_study_samples[0]\n    print(\"\\n--- Case Found ---\")\n    print(f\"Gold Label: {failed_case['gold_label']}\")\n    print(f\"Model Predict: {failed_case['binary_prediction']}\")\n    print(f\"Raw NLI: {failed_case['nli_prediction']} \")\n    print(\"-\" * 30)\n    print(f\"premise:\\n {failed_case['premise']}\")\n    print(f\"hypothesis:\\n {failed_case['hypothesis']}\")\nelse:\n    print(\"No case found\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T10:03:17.286489Z","iopub.execute_input":"2025-11-30T10:03:17.286745Z","iopub.status.idle":"2025-11-30T10:03:17.292116Z","shell.execute_reply.started":"2025-11-30T10:03:17.286729Z","shell.execute_reply":"2025-11-30T10:03:17.291363Z"}},"outputs":[{"name":"stdout","text":"\n--- Case Found ---\nGold Label: Non-Factual\nModel Predict: Factual\nRaw NLI: entailment \n------------------------------\npremise:\n Admiral of the Fleet Matthew Aylmer, 1st Baron Aylmer (ca. 1650 - 18 August 1720) was a Royal Navy officer. He was one of the captains who sent a letter to Prince William of Orange, who had just landed at Torbay, assuring the Prince of the captains' support; the Prince's response ultimately led to the Royal Navy switching allegiance to the Prince and the Glorious Revolution of November 1688. Aylmer saw action at the Battle of Bantry Bay in May 1689, at the Battle of Beachy Head in July 1690, and again at the Battle of Barfleur in May 1692 during the Nine Years' War. Aylmer became Commander-in-Chief of the Navy on 12 November 1709. However, when Aylmer met a French squadron and convoy, he was only able to capture one merchantman and the 56-gun \"Superbe\": the new Harley ministry used this failure as an excuse to remove him as Commander-in-Chief and did so a few months later. Following the accession of George I and the appointment of the Townshend ministry, Aylmer was reappointed Commander-in-Chief on 5 November 1714. He was also appointed Governor of Greenwich Hospital: in this post he founded the Royal Hospital School for the sons of seamen.\nhypothesis:\n He was born in Dublin, the son of a barrister, and was educated at Trinity College, Dublin.\n","output_type":"stream"}],"execution_count":42},{"cell_type":"code","source":"# Code ends here!\n# Below is just a try for RoBERT, which is such computationally expensive, so I gave up to use it","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T06:29:22.089008Z","iopub.execute_input":"2025-12-01T06:29:22.089273Z","iopub.status.idle":"2025-12-01T06:29:22.093559Z","shell.execute_reply.started":"2025-12-01T06:29:22.089255Z","shell.execute_reply":"2025-12-01T06:29:22.092895Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"from datasets import load_dataset\nnli_train_dataset = load_dataset(\"multi_nli\", split=\"train\")\nprint(f\"MultiNLI dataset has loaded，sample size：{len(nli_train_dataset)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T14:26:27.526098Z","iopub.execute_input":"2025-11-30T14:26:27.526819Z","iopub.status.idle":"2025-11-30T14:26:30.602356Z","shell.execute_reply.started":"2025-11-30T14:26:27.526796Z","shell.execute_reply":"2025-11-30T14:26:30.601708Z"}},"outputs":[{"name":"stdout","text":"MultiNLI dataset has loaded，sample size：392702\n","output_type":"stream"}],"execution_count":79},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification, AutoTokenizer\nimport torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T14:26:32.399205Z","iopub.execute_input":"2025-11-30T14:26:32.399914Z","iopub.status.idle":"2025-11-30T14:26:32.403233Z","shell.execute_reply.started":"2025-11-30T14:26:32.399891Z","shell.execute_reply":"2025-11-30T14:26:32.402605Z"}},"outputs":[],"execution_count":80},{"cell_type":"code","source":"MODEL_NAME2 = 'bert-base-uncased'\nrobert_tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME2)\nrobert_model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME2, num_labels=3)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nrobert_model.to(device)\nprint(f\"Current device used: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T14:26:34.275358Z","iopub.execute_input":"2025-11-30T14:26:34.275630Z","iopub.status.idle":"2025-11-30T14:26:35.344785Z","shell.execute_reply.started":"2025-11-30T14:26:34.275611Z","shell.execute_reply":"2025-11-30T14:26:35.344155Z"}},"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Current device used: cuda\n","output_type":"stream"}],"execution_count":81},{"cell_type":"code","source":"from datasets import Dataset\nimport random\n\ntotal_size = len(nli_train_dataset)\nsubset_percentage = 0.05 # 0.01 has been tested, which is so small, very bad performance \nsubset_size = int(total_size * subset_percentage) \n\nnli_train_subset = nli_train_dataset.select(range(subset_size)) \n\nprint(f\"Initial Dataset size: {total_size}. Subset used size: {subset_size}.\")\n\ntokenized_inputs = {\n    \"input_ids\": [],\n    \"attention_mask\": [],\n    \"labels\": [],\n}\n\nprint(f\"Tokenization Start，sample size in total: {len(nli_train_subset)}\")\n\nfor i, example in enumerate(nli_train_subset): \n    encoding = robert_tokenizer(\n        example[\"premise\"], \n        example[\"hypothesis\"], \n        truncation=True, \n        max_length=512,\n        padding=\"max_length\"\n    )\n    \n    tokenized_inputs[\"input_ids\"].append(encoding[\"input_ids\"])\n    tokenized_inputs[\"attention_mask\"].append(encoding[\"attention_mask\"])\n    \n    tokenized_inputs[\"labels\"].append(example[\"label\"]) \n    \n    if (i + 1) % 500 == 0:\n        print(f\"Have processed {i + 1}/{subset_size} samples...\")\n\nprint(\"Tokenization finished...\")\n\ntokenized_train_dataset = Dataset.from_dict(tokenized_inputs)\n\n# tokenized_train_dataset = tokenized_train_dataset.rename_column(\"labels\", \"labels\")\n\ntokenized_train_dataset.set_format(\"torch\")\n\nprint(\"Dataset Preprocessing Finished\")\n\ntraining_args = TrainingArguments(\n    output_dir=\"./roberta_nli_finetuned\",\n    num_train_epochs=NEW_EPOCHS,\n    per_device_train_batch_size=NEW_BATCH_SIZE,\n    dataloader_num_workers=0,\n    warmup_steps=500,\n    weight_decay=0.01,\n    logging_dir='./logs',\n    logging_steps=1000,\n    save_total_limit=1,\n    report_to=\"none\", \n)\n\ntrainer = Trainer(\n    model=robert_model,\n    args=training_args,\n    train_dataset=tokenized_train_dataset, \n    tokenizer=robert_tokenizer,\n    data_collator=DataCollatorWithPadding(tokenizer=robert_tokenizer), \n    compute_metrics=compute_metrics\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T15:41:18.018699Z","iopub.execute_input":"2025-11-30T15:41:18.019260Z","iopub.status.idle":"2025-11-30T15:41:30.308395Z","shell.execute_reply.started":"2025-11-30T15:41:18.019234Z","shell.execute_reply":"2025-11-30T15:41:30.307611Z"}},"outputs":[{"name":"stdout","text":"Initial Dataset size: 392702. Subset used size: 19635.\nTokenization Start，sample size in total: 19635\nHave processed 500/19635 samples...\nHave processed 1000/19635 samples...\nHave processed 1500/19635 samples...\nHave processed 2000/19635 samples...\nHave processed 2500/19635 samples...\nHave processed 3000/19635 samples...\nHave processed 3500/19635 samples...\nHave processed 4000/19635 samples...\nHave processed 4500/19635 samples...\nHave processed 5000/19635 samples...\nHave processed 5500/19635 samples...\nHave processed 6000/19635 samples...\nHave processed 6500/19635 samples...\nHave processed 7000/19635 samples...\nHave processed 7500/19635 samples...\nHave processed 8000/19635 samples...\nHave processed 8500/19635 samples...\nHave processed 9000/19635 samples...\nHave processed 9500/19635 samples...\nHave processed 10000/19635 samples...\nHave processed 10500/19635 samples...\nHave processed 11000/19635 samples...\nHave processed 11500/19635 samples...\nHave processed 12000/19635 samples...\nHave processed 12500/19635 samples...\nHave processed 13000/19635 samples...\nHave processed 13500/19635 samples...\nHave processed 14000/19635 samples...\nHave processed 14500/19635 samples...\nHave processed 15000/19635 samples...\nHave processed 15500/19635 samples...\nHave processed 16000/19635 samples...\nHave processed 16500/19635 samples...\nHave processed 17000/19635 samples...\nHave processed 17500/19635 samples...\nHave processed 18000/19635 samples...\nHave processed 18500/19635 samples...\nHave processed 19000/19635 samples...\nHave processed 19500/19635 samples...\nTokenization finished...\nDataset Preprocessing Finished\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_47/3120362232.py:61: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n","output_type":"stream"}],"execution_count":107},{"cell_type":"code","source":"# !pip3 install evaluate\nfrom transformers import TrainingArguments, Trainer,DataCollatorWithPadding\nimport numpy as np\nimport evaluate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T15:10:28.892671Z","iopub.execute_input":"2025-11-30T15:10:28.893284Z","iopub.status.idle":"2025-11-30T15:10:28.896808Z","shell.execute_reply.started":"2025-11-30T15:10:28.893258Z","shell.execute_reply":"2025-11-30T15:10:28.895985Z"}},"outputs":[],"execution_count":92},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T15:41:33.612712Z","iopub.execute_input":"2025-11-30T15:41:33.613400Z","iopub.status.idle":"2025-11-30T16:25:10.399164Z","shell.execute_reply.started":"2025-11-30T15:41:33.613377Z","shell.execute_reply":"2025-11-30T16:25:10.398543Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1228' max='1228' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1228/1228 43:33, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1000</td>\n      <td>0.367300</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":108,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1228, training_loss=0.3435480726658327, metrics={'train_runtime': 2615.8279, 'train_samples_per_second': 15.012, 'train_steps_per_second': 0.469, 'total_flos': 1.033246391417856e+16, 'train_loss': 0.3435480726658327, 'epoch': 2.0})"},"metadata":{}}],"execution_count":108},{"cell_type":"code","source":"from datasets import Dataset\nimport evaluate\nimport numpy as np\nfrom transformers import DataCollatorWithPadding\n\nLABEL_TO_ID = {\n    \"contradiction\": 0,\n    \"entailment\": 1,\n    \"neutral\": 2\n}\n\ndef map_labels_to_ids(data_list):\n    mapped_data = []\n    for item in data_list:\n        if item['gold_label'] in LABEL_TO_ID:\n            item['labels'] = LABEL_TO_ID[item['gold_label']]\n            mapped_data.append(item)\n    return mapped_data\n\nmapped_matched_data = map_labels_to_ids(matched_data)\nhf_matched_data = Dataset.from_list(mapped_matched_data)\n\ndef tokenize_eval_data(examples):\n    return robert_tokenizer(\n        examples[\"premise\"], \n        examples[\"hypothesis\"], \n        truncation=True, \n        max_length=512\n    )\n\ntokenized_matched_eval = hf_matched_data.map(tokenize_eval_data, batched=True)\ntokenized_matched_eval = tokenized_matched_eval.remove_columns([col for col in tokenized_matched_eval.column_names if col not in ['labels', 'input_ids', 'attention_mask']])\ntokenized_matched_eval.set_format(\"torch\")\n\n\nmapped_mismatched_data = map_labels_to_ids(mismatched_data)\nhf_mismatched_data = Dataset.from_list(mapped_mismatched_data)\n\ntokenized_mismatched_eval = hf_mismatched_data.map(tokenize_eval_data, batched=True)\ntokenized_mismatched_eval = tokenized_mismatched_eval.remove_columns([col for col in tokenized_mismatched_eval.column_names if col not in ['labels', 'input_ids', 'attention_mask']])\ntokenized_mismatched_eval.set_format(\"torch\")\n\n\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    predictions = np.argmax(predictions, axis=1)\n    \n    accuracy_metric = evaluate.load(\"accuracy\")\n    return accuracy_metric.compute(predictions=predictions, references=labels)\n\n\nmatched_results = trainer.evaluate(eval_dataset=tokenized_matched_eval) \nmismatched_results = trainer.evaluate(eval_dataset=tokenized_mismatched_eval) \n\nprint(\"\\n--- RoBERTa NLI Result ---\")\nprint(f\"Matched Set Accuracy: {matched_results.get('eval_accuracy'):.4f}\")\nprint(f\"Mismatched Set Accuracy: {mismatched_results.get('eval_accuracy'):.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T16:28:03.814288Z","iopub.execute_input":"2025-11-30T16:28:03.814794Z","iopub.status.idle":"2025-11-30T16:28:20.623592Z","shell.execute_reply.started":"2025-11-30T16:28:03.814770Z","shell.execute_reply":"2025-11-30T16:28:20.622845Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2460 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7a0415d7ffc744a29336160d987e687c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2464 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c874de34f8254303a8b819b8bba2c5ac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"\n--- RoBERTa NLI Result ---\nMatched Set Accuracy: 0.1171\nMismatched Set Accuracy: 0.1246\n","output_type":"stream"}],"execution_count":109},{"cell_type":"code","source":"\n\ndef tokenize_hallucination_data(examples):\n    return robert_tokenizer(\n        examples[\"premise\"], \n        examples[\"hypothesis\"], \n        truncation=True, \n        max_length=512\n    )\n\nfrom datasets import Dataset\nhf_hallucination_data = Dataset.from_list(evaluation_data)\n\ntokenized_hallucination_eval = hf_hallucination_data.map(tokenize_hallucination_data, batched=True)\n\ntokenized_hallucination_eval = tokenized_hallucination_eval.remove_columns([col for col in tokenized_hallucination_eval.column_names if col not in ['gold_label', 'input_ids', 'attention_mask']])\ntokenized_hallucination_eval = tokenized_hallucination_eval.rename_column(\"gold_label\", \"labels\")\ntokenized_hallucination_eval.set_format(\"torch\", columns=['input_ids', 'attention_mask'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T16:34:20.588080Z","iopub.execute_input":"2025-11-30T16:34:20.588363Z","iopub.status.idle":"2025-11-30T16:34:22.556607Z","shell.execute_reply.started":"2025-11-30T16:34:20.588344Z","shell.execute_reply":"2025-11-30T16:34:22.556022Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1908 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"842108e0433945a6a007fb48d2d392a6"}},"metadata":{}}],"execution_count":112},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, precision_recall_fscore_support\nfrom typing import List","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T16:41:07.550928Z","iopub.execute_input":"2025-11-30T16:41:07.551421Z","iopub.status.idle":"2025-11-30T16:41:07.555024Z","shell.execute_reply.started":"2025-11-30T16:41:07.551399Z","shell.execute_reply":"2025-11-30T16:41:07.554271Z"}},"outputs":[],"execution_count":119},{"cell_type":"code","source":"# 1: Entailment (Factual), 0: Contradiction (Non-Factual), 2: Neutral (Non-Factual)\nID_TO_BINARY_LABEL = {\n    1: 'Factual',\n    0: 'Non-Factual',\n    2: 'Non-Factual'\n}\n\ndef convert_nli_to_binary(nli_predictions: np.ndarray) -> list:\n    binary_preds = []\n    predicted_ids = np.argmax(nli_predictions, axis=1)\n    \n    for pred_id in predicted_ids:\n        binary_preds.append(ID_TO_BINARY_LABEL[pred_id])\n        \n    return binary_preds\n\nprediction_output = trainer.predict(tokenized_hallucination_eval)\n\nnli_predictions = prediction_output.predictions\n\nroberta_binary_preds = convert_nli_to_binary(nli_predictions)\nroberta_true_labels = hf_hallucination_data['gold_label'] \n\nroberta_hallucination_metrics = calculate_metrics(roberta_true_labels, roberta_binary_preds)\n\nprint(\"\\n--- RoBERTa Hallucination Detection Result ---\")\nfor metric, value in roberta_hallucination_metrics.items():\n    print(f\"{metric.capitalize()}: {value:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T16:41:10.524378Z","iopub.execute_input":"2025-11-30T16:41:10.524642Z","iopub.status.idle":"2025-11-30T16:41:33.441850Z","shell.execute_reply.started":"2025-11-30T16:41:10.524622Z","shell.execute_reply":"2025-11-30T16:41:33.441030Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"\n--- RoBERTa Hallucination Detection Result ---\nAccuracy: 0.2935\nPrecision: 1.0000\nRecall: 0.2935\nF1: 0.4538\n","output_type":"stream"}],"execution_count":120},{"cell_type":"code","source":"def analyze_case_robert(premise: str, hypothesis: str, trainer):\n    test_data = [{'premise': premise, 'hypothesis': hypothesis}]\n    \n    def tokenize_case(examples):\n        return robert_tokenizer(\n            examples[\"premise\"], \n            examples[\"hypothesis\"], \n            truncation=True, \n            max_length=512\n        )\n\n    from datasets import Dataset\n    from transformers import DataCollatorWithPadding\n\n    hf_test_data = Dataset.from_list(test_data).map(tokenize_case, batched=True)\n    \n    hf_test_data = hf_test_data.remove_columns([col for col in hf_test_data.column_names if col not in ['input_ids', 'attention_mask']])\n    hf_test_data.set_format(\"torch\")\n    \n    prediction_output = trainer.predict(test_dataset=hf_test_data)\n    predicted_id = np.argmax(prediction_output.predictions[0])\n    \n    # 0, 1, 2 projected to Contradiction, Entailment, Neutral\n    NLI_ID_TO_LABEL = {0: \"contradiction\", 1: \"entailment\", 2: \"neutral\"}\n    \n    predicted_label = NLI_ID_TO_LABEL[predicted_id]\n    \n    return predicted_label\n\nFLAN_FAILED_PREMISE = \"Admiral of the Fleet Matthew Aylmer, 1st Baron Aylmer (ca. 1650 - 18 August 1720) was a Royal Navy officer. He was one of the captains who sent a letter to Prince William of Orange, who had just landed at Torbay, assuring the Prince of the captains' support; the Prince's response ultimately led to the Royal Navy switching allegiance to the Prince and the Glorious Revolution of November 1688. Aylmer saw action at the Battle of Bantry Bay in May 1689, at the Battle of Beachy Head in July 1690, and again at the Battle of Barfleur in May 1692 during the Nine Years' War. Aylmer became Commander-in-Chief of the Navy on 12 November 1709. However, when Aylmer met a French squadron and convoy, he was only able to capture one merchantman and the 56-gun \\\"Superbe\\\": the new Harley ministry used this failure as an excuse to remove him as Commander-in-Chief and did so a few months later. Following the accession of George I and the appointment of the Townshend ministry, Aylmer was reappointed Commander-in-Chief on 5 November 1714. He was also appointed Governor of Greenwich Hospital: in this post he founded the Royal Hospital School for the sons of seamen.\"\nFLAN_FAILED_HYPOTHESIS = \"He was born in Dublin, the son of a barrister, and was educated at Trinity College, Dublin.\"\n\nrobert_prediction = analyze_case_robert(FLAN_FAILED_PREMISE, FLAN_FAILED_HYPOTHESIS, trainer)\n\nprint(f\"Flan-T5 prediction: Entailment (Factual)\")\nprint(f\"RoBERTa prediction: {robert_prediction}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T16:56:29.097793Z","iopub.execute_input":"2025-11-30T16:56:29.098079Z","iopub.status.idle":"2025-11-30T16:56:29.157244Z","shell.execute_reply.started":"2025-11-30T16:56:29.098060Z","shell.execute_reply":"2025-11-30T16:56:29.156662Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8abae8eb73fd41028a0ddb539a184abf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Flan-T5 prediction: Entailment (Factual)\nRoBERTa prediction: entailment\n","output_type":"stream"}],"execution_count":124},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}