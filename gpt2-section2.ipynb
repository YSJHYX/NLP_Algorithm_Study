{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13929563,"sourceType":"datasetVersion","datasetId":8876624}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport json\nimport numpy as np\nfrom typing import List, Dict\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, GPT2Tokenizer\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# device = \"cpu\"\nprint(f\"device used: {device}\")\nMODEL_NAME_GPT2 = \"gpt2\" \n\ngpt2_tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME_GPT2)\ngpt2_tokenizer.pad_token = gpt2_tokenizer.eos_token \n\ngpt2_model = AutoModelForCausalLM.from_pretrained(MODEL_NAME_GPT2)\ngpt2_model.to(device).eval() \n\nVERBALIZER = {\n    \"entailment\": \"true\",\n    \"contradiction\": \"false\",\n    \"neutral\": \"neither\",\n}\nverbalizer_token_ids = {}\n\nfor label, word in VERBALIZER.items():\n    token_id = gpt2_tokenizer.encode(word, add_special_tokens=False)[0]\n    verbalizer_token_ids[label] = token_id\n\nprint(\"\\nVerbalizer Token ID projection created:\", verbalizer_token_ids)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T04:17:21.254330Z","iopub.execute_input":"2025-12-01T04:17:21.255390Z","iopub.status.idle":"2025-12-01T04:18:03.597762Z","shell.execute_reply.started":"2025-12-01T04:17:21.255350Z","shell.execute_reply":"2025-12-01T04:18:03.596921Z"}},"outputs":[{"name":"stdout","text":"device used: cuda\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5504cd327704495d9a96e0cf0cdead3e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e0844d9e83884a648a4f6d3d6ef0be21"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fdffe127cd194c5dab1b885411aedb99"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d3a76cd2ce54371a38adf13bf849f54"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a5db1abe31674350bfecef728585e820"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n2025-12-01 04:17:42.960710: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1764562663.143872      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1764562663.201343      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c68a6ec6e8f4898ad58f6472898a1ab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"64482701541a45dcb275ac0589a5a73a"}},"metadata":{}},{"name":"stdout","text":"\nVerbalizer Token ID projection created: {'entailment': 7942, 'contradiction': 9562, 'neutral': 710}\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"LABEL_MAPPING = {\n    \"entailment\": \"entailment\",\n    \"contradiction\": \"contradiction\",\n    \"neutral\": \"neutral\"\n}\n\ndef load_jsonl_data(file_path: str) -> List[Dict]:\n    data = []\n    with open(file_path, 'r', encoding='utf-8') as f:\n        for line in f:\n            try:\n                item = json.loads(line.strip())\n                if item.get(\"gold_label\") in LABEL_MAPPING:\n                    data.append({\n                        'premise': item['sentence1'],  \n                        'hypothesis': item['sentence2'], \n                        'gold_label': item['gold_label']\n                    })\n            except (json.JSONDecodeError, KeyError) as e:\n                continue \n    return data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T04:18:07.492948Z","iopub.execute_input":"2025-12-01T04:18:07.493574Z","iopub.status.idle":"2025-12-01T04:18:07.499410Z","shell.execute_reply.started":"2025-12-01T04:18:07.493552Z","shell.execute_reply":"2025-12-01T04:18:07.498505Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"def predict_nli_gpt2(premise: str, hypothesis: str, model, tokenizer, device, verbalizer_token_ids) -> str:\n    query = f'Premise：“{premise}”. Hypothesis：“{hypothesis}”. The relationship is：'\n    \n    inputs = tokenizer(\n        query, \n        return_tensors=\"pt\",\n        truncation=True, \n        max_length=1024,\n        padding=\"longest\"\n    )\n    inputs = {k: v.to(device) for k, v in inputs.items()}\n    \n    with torch.no_grad(): \n        outputs = model(**inputs) \n        \n    logits = outputs.logits[0, -1, :]  \n    probabilities = torch.softmax(logits, dim=-1)\n    max_prob = -1.0\n    predicted_label = \"neutral\" \n    \n    for label, token_id in verbalizer_token_ids.items():\n        prob = probabilities[token_id].item() \n        \n        if prob > max_prob:\n            max_prob = prob\n            predicted_label = label\n            \n    return predicted_label\n\n\ndef run_evaluation(data: List[Dict], set_name: str, model, tokenizer, verbalizer_token_ids):\n    correct_predictions = 0\n    total_samples = len(data)\n\n    print(f\"\\n--- Evaluation Starting {set_name} (Total samples: {total_samples}) ---\")\n\n    for i, sample in enumerate(data):\n        predicted_label = predict_nli_gpt2(\n            sample['premise'], \n            sample['hypothesis'], \n            model, \n            tokenizer, \n            device, \n            verbalizer_token_ids\n        )\n\n        if predicted_label == sample['gold_label']:\n            correct_predictions += 1\n            \n        if (i + 1) % 500 == 0:\n            print(f\"Have processed {i + 1}/{total_samples} samples...\")\n\n    accuracy = correct_predictions / total_samples\n    print(f\"--- Evaluation Finished {set_name} ---\")\n    print(f\"Accuracy: {accuracy:.4f} ({correct_predictions} / {total_samples})\")\n    \n    return accuracy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T04:18:12.356238Z","iopub.execute_input":"2025-12-01T04:18:12.356788Z","iopub.status.idle":"2025-12-01T04:18:12.364750Z","shell.execute_reply.started":"2025-12-01T04:18:12.356765Z","shell.execute_reply":"2025-12-01T04:18:12.363983Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"MATCHED_FILE = \"/kaggle/input/nlpindividualproject/dev_matched_sampled-1.jsonl\"\nMISMATCHED_FILE = \"/kaggle/input/nlpindividualproject/dev_mismatched_sampled-1.jsonl\"\n\nmatched_data = load_jsonl_data(MATCHED_FILE)\nmismatched_data = load_jsonl_data(MISMATCHED_FILE)\n\nmatched_acc = run_evaluation(matched_data, \"Matched Set\", gpt2_model, gpt2_tokenizer, verbalizer_token_ids)\nmismatched_acc = run_evaluation(mismatched_data, \"Mismatched Set\", gpt2_model, gpt2_tokenizer, verbalizer_token_ids)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T04:18:20.897893Z","iopub.execute_input":"2025-12-01T04:18:20.898632Z","iopub.status.idle":"2025-12-01T04:19:09.745729Z","shell.execute_reply.started":"2025-12-01T04:18:20.898594Z","shell.execute_reply":"2025-12-01T04:19:09.744915Z"}},"outputs":[{"name":"stdout","text":"\n--- Evaluation Starting Matched Set (Total samples: 2460) ---\nHave processed 500/2460 samples...\nHave processed 1000/2460 samples...\nHave processed 1500/2460 samples...\nHave processed 2000/2460 samples...\n--- Evaluation Finished Matched Set ---\nAccuracy: 0.3415 (840 / 2460)\n\n--- Evaluation Starting Mismatched Set (Total samples: 2464) ---\nHave processed 500/2464 samples...\nHave processed 1000/2464 samples...\nHave processed 1500/2464 samples...\nHave processed 2000/2464 samples...\n--- Evaluation Finished Mismatched Set ---\nAccuracy: 0.3584 (883 / 2464)\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"from datasets import load_dataset\n\nhallucination_dataset = load_dataset(\"potsawee/wiki_bio_gpt3_hallucination\", split=\"evaluation\")\n\n\ndef preprocess_hallucination_data(dataset) -> List[Dict]:\n    processed_samples = []\n    \n    for entry in dataset:\n        premise = entry['wiki_bio_text']\n        gpt3_sentences = entry['gpt3_sentences']\n        annotations = entry['annotation']\n        \n        for sentence, annotation in zip(gpt3_sentences, annotations):\n            binary_label = 'Factual' if annotation == 0.0 else 'Non-Factual'    \n            processed_samples.append({\n                'premise': premise,\n                'hypothesis': sentence,\n                'gold_label': binary_label \n            })\n            \n    return processed_samples\nif hallucination_dataset:\n    evaluation_data = preprocess_hallucination_data(hallucination_dataset)\n    print(f\"Dataset preprocessing finished，total {len(evaluation_data)} sentence evaluation samples.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T04:19:13.782479Z","iopub.execute_input":"2025-12-01T04:19:13.783146Z","iopub.status.idle":"2025-12-01T04:19:18.627867Z","shell.execute_reply.started":"2025-12-01T04:19:13.783113Z","shell.execute_reply":"2025-12-01T04:19:18.627103Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"68c2bf36044045a695c247b636b65ee0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/evaluation-00000-of-00001-e91191b8f(…):   0%|          | 0.00/2.56M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"774c5ac03b744134aaa6aab64a3c7e53"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating evaluation split:   0%|          | 0/238 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d5fc43376fb348768d06865cb04527be"}},"metadata":{}},{"name":"stdout","text":"Dataset preprocessing finished，total 1908 sentence evaluation samples.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, precision_recall_fscore_support","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T04:19:22.408943Z","iopub.execute_input":"2025-12-01T04:19:22.410139Z","iopub.status.idle":"2025-12-01T04:19:22.413673Z","shell.execute_reply.started":"2025-12-01T04:19:22.410109Z","shell.execute_reply":"2025-12-01T04:19:22.412784Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, precision_recall_fscore_support\nfrom typing import List\n\ndef calculate_metrics(y_true: List[str], y_pred: List[str]):\n    accuracy = accuracy_score(y_true, y_pred)\n    precision, recall, f1, _ = precision_recall_fscore_support(\n        y_true, \n        y_pred, \n        average='binary', \n        pos_label='Non-Factual', \n        zero_division=0\n    )\n\n    return {\n        'accuracy': accuracy,\n        'precision': precision,\n        'recall': recall,\n        'f1': f1\n    }\n\nprint(\"calculate_metrics is used\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T04:21:42.306563Z","iopub.execute_input":"2025-12-01T04:21:42.307228Z","iopub.status.idle":"2025-12-01T04:21:42.312483Z","shell.execute_reply.started":"2025-12-01T04:21:42.307201Z","shell.execute_reply":"2025-12-01T04:21:42.311807Z"}},"outputs":[{"name":"stdout","text":"calculate_metrics is used\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"\ntry:\n    hallucination_dataset = load_dataset(\"potsawee/wiki_bio_gpt3_hallucination\", split=\"evaluation\")\nexcept Exception:\n    hallucination_dataset = None\n\nif hallucination_dataset:\n    evaluation_data = preprocess_hallucination_data(hallucination_dataset)\nelse:\n    evaluation_data = []\n\ndef run_hallucination_evaluation(data: List[Dict], model, tokenizer, device, verbalizer_token_ids):\n    y_true = []\n    y_pred = []\n    total_samples = len(data)\n    \n    for i, sample in enumerate(data):\n        nli_predicted_label = predict_nli_gpt2(\n            sample['premise'], \n            sample['hypothesis'], \n            model, \n            tokenizer, \n            device,\n            verbalizer_token_ids\n        )\n        \n        binary_predicted_label = 'Factual' if nli_predicted_label == 'entailment' else 'Non-Factual'\n        \n        y_pred.append(binary_predicted_label)\n        y_true.append(sample['gold_label'])\n\n    if not y_true:\n        print(\"NO SAMPLE FOUND\")\n        return None\n\n    metrics = calculate_metrics(y_true, y_pred)\n    return metrics\n\ngpt2_hallucination_metrics = run_hallucination_evaluation(\n    evaluation_data, \n    gpt2_model, \n    gpt2_tokenizer,\n    device,\n    verbalizer_token_ids\n)\n\nif gpt2_hallucination_metrics:\n    print(\"\\n--- GPT-2 halluciantion detection results ---\")\n    for metric, value in gpt2_hallucination_metrics.items():\n        print(f\"{metric.capitalize()}: {value:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T04:21:48.198408Z","iopub.execute_input":"2025-12-01T04:21:48.198667Z","iopub.status.idle":"2025-12-01T04:22:31.891351Z","shell.execute_reply.started":"2025-12-01T04:21:48.198650Z","shell.execute_reply":"2025-12-01T04:22:31.890581Z"}},"outputs":[{"name":"stdout","text":"\n--- GPT-2 halluciantion detection results ---\nAccuracy: 0.8905\nPrecision: 1.0000\nRecall: 0.8905\nF1: 0.9421\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"FLAN_FAILED_PREMISE = \"Admiral of the Fleet Matthew Aylmer, 1st Baron Aylmer (ca. 1650 - 18 August 1720) was a Royal Navy officer. He was one of the captains who sent a letter to Prince William of Orange, who had just landed at Torbay, assuring the Prince of the captains' support; the Prince's response ultimately led to the Royal Navy switching allegiance to the Prince and the Glorious Revolution of November 1688. Aylmer saw action at the Battle of Bantry Bay in May 1689, at the Battle of Beachy Head in July 1690, and again at the Battle of Barfleur in May 1692 during the Nine Years' War. Aylmer became Commander-in-Chief of the Navy on 12 November 1709. However, when Aylmer met a French squadron and convoy, he was only able to capture one merchantman and the 56-gun \\\"Superbe\\\": the new Harley ministry used this failure as an excuse to remove him as Commander-in-Chief and did so a few months later. Following the accession of George I and the appointment of the Townshend ministry, Aylmer was reappointed Commander-in-Chief on 5 November 1714. He was also appointed Governor of Greenwich Hospital: in this post he founded the Royal Hospital School for the sons of seamen.\"\nFLAN_FAILED_HYPOTHESIS = \"He was born in Dublin, the son of a barrister, and was educated at Trinity College, Dublin.\"\n\ngpt2_case_prediction = predict_nli_gpt2(\n    FLAN_FAILED_PREMISE, \n    FLAN_FAILED_HYPOTHESIS, \n    gpt2_model, \n    gpt2_tokenizer, \n    device, \n    verbalizer_token_ids\n)\n\nprint(f\"Gold Label: Neutral\")\nprint(f\"Flan-T5 prediction (Prompting): Entailment\")\nprint(f\"GPT-2 prediction (Prompting): {gpt2_case_prediction}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T04:28:13.004807Z","iopub.execute_input":"2025-12-01T04:28:13.005392Z","iopub.status.idle":"2025-12-01T04:28:13.033428Z","shell.execute_reply.started":"2025-12-01T04:28:13.005368Z","shell.execute_reply":"2025-12-01T04:28:13.032832Z"}},"outputs":[{"name":"stdout","text":"Gold Label: Neutral\nFlan-T5 prediction (Prompting): Entailment\nGPT-2 prediction (Prompting): neutral\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}